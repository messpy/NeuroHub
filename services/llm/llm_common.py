#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
services/llm/llm_common.py
- å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£: .env ãƒ­ãƒ¼ãƒ‰ / config.yaml ãƒ­ãƒ¼ãƒ‰ / Ollamaãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
- å‰æ: .env ã¯ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç›´ä¸‹ã«é…ç½®ã™ã‚‹ï¼ˆä¾‹: ~/work/NeuroHub/.envï¼‰
"""
from __future__ import annotations
import os
import json
import time
from pathlib import Path
from typing import Any, Dict, Optional
from dataclasses import dataclass, field

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’åŸºç‚¹ã«å›ºå®š
PROJECT_ROOT = Path(__file__).resolve().parents[2]
CONFIG_DIR = PROJECT_ROOT / "config"
YAML_FILE = CONFIG_DIR / "config.yaml"
PROMPT_TEMPLATES_FILE = CONFIG_DIR / "prompt_templates.yaml"
ENV_FILE = PROJECT_ROOT / ".env"


# ==========================================================
# .env ãƒ­ãƒ¼ãƒ‰
# ==========================================================
def load_env_from_config(debug: bool = False) -> None:
    """
    ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç›´ä¸‹ (.env) ã‚’èª­ã¿è¾¼ã‚€ã ã‘ã®æœ€å°å®Ÿè£…ã€‚
    ä¾‹: ~/work/NeuroHub/.env
    """
    try:
        from dotenv import load_dotenv
    except ImportError:
        if debug:
            print("[llm_common] python-dotenv æœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã€‚ç’°å¢ƒèª­ã¿è¾¼ã¿ã‚¹ã‚­ãƒƒãƒ—ã€‚", flush=True)
        return

    if ENV_FILE.exists():
        load_dotenv(ENV_FILE)
        if debug:
            print(f"[llm_common] loaded .env: {ENV_FILE}", flush=True)
    else:
        if debug:
            print(f"[llm_common] .env not found: {ENV_FILE}", flush=True)


# ==========================================================
# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆèª­ã¿è¾¼ã¿
# ==========================================================
def load_prompt_templates(path: Path | None = None) -> Dict[str, Any]:
    """prompt_templates.yaml ã‚’è¾æ›¸ã§è¿”ã™ï¼ˆç„¡ã‘ã‚Œã° {}ï¼‰ã€‚"""
    import yaml
    p = Path(path) if path else PROMPT_TEMPLATES_FILE
    if not p.exists():
        return {}
    try:
        with p.open("r", encoding="utf-8") as f:
            return yaml.safe_load(f) or {}
    except Exception as e:
        print(f"[llm_common] warn: failed to read prompt templates: {e}", flush=True)
        return {}


def get_prompt_template(template_type: str, template_name: str, **kwargs) -> str:
    """
    ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’å–å¾—ã—ã¦ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ

    Args:
        template_type: ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚¿ã‚¤ãƒ— (ä¾‹: 'git_commit', 'code_analysis')
        template_name: ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå (ä¾‹: 'base_prompt', 'detailed_prompt')
        **kwargs: ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå†…ã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’ç½®æ›ã™ã‚‹å€¤

    Returns:
        ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ¸ˆã¿ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ–‡å­—åˆ—
    """
    templates = load_prompt_templates()

    try:
        template = templates.get("prompts", {}).get(template_type, {}).get(template_name, "")
        if not template:
            return f"ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {template_type}.{template_name}"

        # {base_prompt} ãªã©ã®å‚ç…§ã‚’è§£æ±º
        if "{base_prompt}" in template:
            base_prompt = templates.get("prompts", {}).get(template_type, {}).get("base_prompt", "")
            kwargs["base_prompt"] = base_prompt

        return template.format(**kwargs)
    except Exception as e:
        return f"ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}"


def get_system_message(message_type: str) -> str:
    """ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’å–å¾—"""
    templates = load_prompt_templates()
    return templates.get("prompts", {}).get("system_messages", {}).get(message_type, "")


def get_api_defaults(provider: str) -> Dict[str, Any]:
    """ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åˆ¥ã®API ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’å–å¾—"""
    templates = load_prompt_templates()
    return templates.get("api_defaults", {}).get(provider, {})
def load_config(path: Path | None = None) -> Dict[str, Any]:
    """config/config.yaml ã‚’è¾æ›¸ã§è¿”ã™ï¼ˆç„¡ã‘ã‚Œã° {}ï¼‰ã€‚"""
    import yaml
    p = Path(path) if path else YAML_FILE
    if not p.exists():
        return {}
    try:
        with p.open("r", encoding="utf-8") as f:
            return yaml.safe_load(f) or {}
    except Exception as e:
        print(f"[llm_common] warn: failed to read yaml: {e}", flush=True)
        return {}


# ==========================================================
# çµ±ä¸€ãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼
# ==========================================================
@dataclass
class LLMResponse:
    """
    LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‹ã‚‰ã®çµ±ä¸€ãƒ¬ã‚¹ãƒãƒ³ã‚¹å½¢å¼
    é–¢æ•°å‘¼ã³å‡ºã—ã‚„å¤–éƒ¨ã‚·ã‚¹ãƒ†ãƒ ã§ã‚‚åˆ©ç”¨å¯èƒ½ãªè©³ç´°æƒ…å ±ã‚’å«ã‚€
    """
    # åŸºæœ¬æƒ…å ±
    status_code: int                    # HTTPã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ï¼ˆ200, 400, 401ãªã©ï¼‰
    provider: str                       # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åï¼ˆollama, gemini, huggingfaceï¼‰
    model: str                          # ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«å
    content: str                        # ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆå†…å®¹
    error: Optional[str] = None         # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼ˆã‚ã‚‹å ´åˆï¼‰

    # è©³ç´°ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆé–¢æ•°å‘¼ã³å‡ºã—ç­‰ã§æ´»ç”¨ï¼‰
    metadata: Dict[str, Any] = field(default_factory=dict)

    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æƒ…å ±
    response_time: Optional[float] = None       # ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ï¼ˆç§’ï¼‰
    request_timestamp: Optional[str] = None     # ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ™‚åˆ»

    # ãƒˆãƒ¼ã‚¯ãƒ³æƒ…å ±ï¼ˆåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
    tokens_used: Optional[int] = None           # ä½¿ç”¨ãƒˆãƒ¼ã‚¯ãƒ³æ•°
    tokens_input: Optional[int] = None          # å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°
    tokens_output: Optional[int] = None         # å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°

    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±
    request_url: Optional[str] = None           # ãƒªã‚¯ã‚¨ã‚¹ãƒˆURL
    request_payload: Optional[Dict] = None      # ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒšã‚¤ãƒ­ãƒ¼ãƒ‰
    raw_response: Optional[Dict] = None         # ç”Ÿãƒ¬ã‚¹ãƒãƒ³ã‚¹

    @property
    def is_success(self) -> bool:
        """æˆåŠŸã—ãŸã‹ã©ã†ã‹"""
        return self.status_code == 200 and not self.error

    @property
    def is_error(self) -> bool:
        """ã‚¨ãƒ©ãƒ¼ã‹ã©ã†ã‹"""
        return not self.is_success

    def to_dict(self) -> Dict[str, Any]:
        """è¾æ›¸å½¢å¼ã«å¤‰æ›ï¼ˆé–¢æ•°å‘¼ã³å‡ºã—ç”¨ï¼‰"""
        result = {
            'status_code': self.status_code,
            'provider': self.provider,
            'model': self.model,
            'content': self.content,
            'is_success': self.is_success,
            'response_time': self.response_time,
            'request_timestamp': self.request_timestamp
        }

        if self.error:
            result['error'] = self.error

        if self.tokens_used is not None:
            result['tokens_used'] = self.tokens_used

        if self.tokens_input is not None:
            result['tokens_input'] = self.tokens_input

        if self.tokens_output is not None:
            result['tokens_output'] = self.tokens_output

        if self.metadata:
            result['metadata'] = self.metadata

        return result

    def format_for_debug_level(self, debug_level: int) -> str:
        """
        ãƒ‡ãƒãƒƒã‚°ãƒ¬ãƒ™ãƒ«ã«å¿œã˜ã¦ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã•ã‚ŒãŸæ–‡å­—åˆ—ã‚’è¿”ã™
        0: ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ã¿
        1: åŸºæœ¬æƒ…å ±ï¼ˆãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã€ãƒ¢ãƒ‡ãƒ«ã€ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ï¼‰
        2: ãƒˆãƒ¼ã‚¯ãƒ³æƒ…å ±ã‚‚å«ã‚€
        3: å…¨è©³ç´°æƒ…å ±ï¼ˆç§˜å¯†æƒ…å ±ã¯é™¤ãï¼‰
        """
        if debug_level == 0:
            return self.content

        lines = []

        if debug_level >= 1:
            lines.append(f"Provider: {self.provider}")
            lines.append(f"Model: {self.model}")
            lines.append(f"Status: {self.status_code}")
            if self.response_time:
                lines.append(f"Response Time: {self.response_time:.3f}s")
            if self.error:
                lines.append(f"Error: {self.error}")
            lines.append(f"Content: {self.content}")

        if debug_level >= 2:
            if self.tokens_used is not None:
                lines.append(f"Tokens Used: {self.tokens_used}")
            if self.tokens_input is not None:
                lines.append(f"Input Tokens: {self.tokens_input}")
            if self.tokens_output is not None:
                lines.append(f"Output Tokens: {self.tokens_output}")
            if self.request_timestamp:
                lines.append(f"Timestamp: {self.request_timestamp}")

        if debug_level >= 3:
            if self.request_url:
                lines.append(f"Request URL: {self.request_url}")
            if self.metadata:
                lines.append("Metadata:")
                for key, value in self.metadata.items():
                    # ç§˜å¯†æƒ…å ±ã¯é™¤å¤–
                    if not any(secret in key.lower() for secret in ['key', 'token', 'password', 'secret']):
                        lines.append(f"  {key}: {value}")
            if self.request_payload:
                lines.append("Request Payload:")
                # ç§˜å¯†æƒ…å ±ã‚’å«ã‚€å¯èƒ½æ€§ãŒã‚ã‚‹ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¯é™¤å¤–
                safe_payload = {}
                for key, value in self.request_payload.items():
                    if not any(secret in key.lower() for secret in ['key', 'token', 'password', 'secret']):
                        safe_payload[key] = value
                lines.append(f"  {json.dumps(safe_payload, ensure_ascii=False, indent=2)}")

        return "\n".join(lines)


# ==========================================================
# LLMè‡ªå‹•å±¥æ­´è¨˜éŒ²æ©Ÿèƒ½
# ==========================================================
def auto_log_llm_request(func):
    """
    LLMé–¢æ•°å®Ÿè¡Œæ™‚ã«è‡ªå‹•çš„ã«å±¥æ­´ã‚’DBã«è¨˜éŒ²ã™ã‚‹ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿
    """
    def wrapper(*args, **kwargs):
        from ..db.llm_history_manager import LLMHistoryManager

        # é–¢æ•°å®Ÿè¡Œ
        start_time = time.time()
        try:
            result = func(*args, **kwargs)
            success = True
            error_message = None
        except Exception as e:
            result = None
            success = False
            error_message = str(e)

        response_time_ms = int((time.time() - start_time) * 1000)

        # å±¥æ­´ã«è¨˜éŒ²
        try:
            manager = LLMHistoryManager()
            if not manager.current_session_id:
                manager.start_session("auto")

            # å¼•æ•°ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡º
            provider = kwargs.get('provider', 'unknown')
            model = kwargs.get('model', 'unknown')
            prompt = kwargs.get('prompt', str(args[0]) if args else '')
            response_text = str(result) if result else ''

            manager.log_llm_request(
                provider=provider,
                model=model,
                prompt_text=prompt[:1000],  # é•·ã™ãã‚‹å ´åˆã¯åˆ‡ã‚Šè©°ã‚
                response_text=response_text[:1000],
                success=success,
                error_message=error_message,
                response_time_ms=response_time_ms,
                request_type=func.__name__
            )
        except Exception as log_error:
            # ãƒ­ã‚°è¨˜éŒ²ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–ï¼ˆãƒ¡ã‚¤ãƒ³å‡¦ç†ã«å½±éŸ¿ã•ã›ãªã„ï¼‰
            print(f"[auto_log] ãƒ­ã‚°è¨˜éŒ²ã‚¨ãƒ©ãƒ¼: {log_error}", flush=True)

        if not success:
            raise Exception(error_message)

        return result

    return wrapper


def init_llm_database(db_path: str = "neurohub_llm.db") -> bool:
    """
    LLMå±¥æ­´ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’åˆæœŸåŒ–

    Returns:
        åˆæœŸåŒ–æˆåŠŸã®å ´åˆTrue
    """
    try:
        from ..db.llm_history_manager import LLMHistoryManager
        manager = LLMHistoryManager(db_path)
        print(f"[llm_common] LLMå±¥æ­´ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–å®Œäº†: {db_path}")
        return True
    except Exception as e:
        print(f"[llm_common] ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}", flush=True)
        return False
class DebugLogger:
    """æ¥µå°ãƒ‡ãƒãƒƒã‚¬ï¼ˆstderrå‡ºåŠ›ï¼‰ã€‚enabled=False ãªã‚‰ç„¡å‹•ä½œã€‚"""
    def __init__(self, enabled: bool = False, level: int = 1) -> None:
        self.enabled = enabled
        self.level = level  # ãƒ‡ãƒãƒƒã‚°ãƒ¬ãƒ™ãƒ« (0-3)

    def dbg(self, *args) -> None:
        if self.enabled:
            try:
                import sys
                out = " ".join(str(a) for a in args)
                print(f"[debug] {out}", file=sys.stderr)
            except Exception:
                pass

    def log_response(self, response: LLMResponse) -> None:
        """LLMResponseã‚’ãƒ‡ãƒãƒƒã‚°ãƒ¬ãƒ™ãƒ«ã«å¿œã˜ã¦å‡ºåŠ›"""
        if self.enabled:
            try:
                import sys
                formatted_output = response.format_for_debug_level(self.level)
                if self.level == 0:
                    # ãƒ¬ãƒ™ãƒ«0ã®å ´åˆã¯ç›´æ¥ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å‡ºåŠ›
                    print(formatted_output)
                else:
                    # ãƒ¬ãƒ™ãƒ«1ä»¥ä¸Šã®å ´åˆã¯ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã¨ã—ã¦å‡ºåŠ›
                    print(f"[debug] LLM Response (level {self.level}):", file=sys.stderr)
                    for line in formatted_output.split('\n'):
                        print(f"[debug] {line}", file=sys.stderr)
            except Exception:
                pass


# ==========================================================
# ãƒ¬ã‚¹ãƒãƒ³ã‚¹ä½œæˆãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
# ==========================================================
def create_llm_response(
    status_code: int,
    provider: str,
    model: str,
    content: str = "",
    error: Optional[str] = None,
    response_time: Optional[float] = None,
    **kwargs
) -> LLMResponse:
    """LLMResponseã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°"""
    from datetime import datetime

    return LLMResponse(
        status_code=status_code,
        provider=provider,
        model=model,
        content=content,
        error=error,
        response_time=response_time,
        request_timestamp=datetime.now().isoformat(),
        **kwargs
    )


# ==========================================================
# ãƒ¢ãƒ‡ãƒ«åå–å¾—
# ==========================================================
def get_llm_model_from_config(cfg: Dict[str, Any], provider: str, default_model: str) -> str:
    """
    config.yaml ã® llm.<provider>.model ã‚’è¿”ã™ï¼ˆãªã‘ã‚Œã° default_modelï¼‰ã€‚
    ä¾‹:
      llm:
        gemini:
          model: gemini-2.5-flash
    """
    try:
        node = (cfg or {}).get("llm", {}).get(provider, {})
        model = node.get("model") or default_model
        return str(model)
    except Exception:
        return default_model


# ==========================================================
# ã‚ªãƒ—ã‚·ãƒ§ãƒ³è§£æï¼ˆå…±é€šï¼‰
# ==========================================================
def parse_opt_kv(opts: list[str] | None) -> Dict[str, Any]:
    """
    key=value å½¢å¼ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãƒªã‚¹ãƒˆã‚’è¾æ›¸ã«å¤‰æ›ã™ã‚‹å…±é€šé–¢æ•°ã€‚
    JSONé¢¨ã®å€¤ã€booleanã€æ•°å€¤ã®è‡ªå‹•å¤‰æ›ã‚’è¡Œã†ã€‚
    """
    if not opts:
        return {}

    import json
    out: Dict[str, Any] = {}
    for kv in opts:
        if "=" not in kv:
            continue
        k, v = kv.split("=", 1)
        k, v = k.strip(), v.strip()

        # JSONé¢¨ã‚’å„ªå…ˆ
        try:
            if (v.startswith("{") and v.endswith("}")) or (v.startswith("[") and v.endswith("]")) or v in ("true","false","null"):
                out[k] = json.loads(v.replace("'", '"'))
                continue
        except Exception:
            pass

        # booleanåˆ¤å®š
        if v.lower() in ("true","false"):
            out[k] = (v.lower() == "true")
            continue

        # æ•°å€¤åˆ¤å®šï¼ˆint â†’ float ã®é †ï¼‰
        try:
            out[k] = int(v)
            continue
        except ValueError:
            pass
        try:
            out[k] = float(v)
            continue
        except ValueError:
            pass

        # æ–‡å­—åˆ—ã¨ã—ã¦æ‰±ã†
        out[k] = v
    return out


# ==========================================================
# ç’°å¢ƒç¢ºèªï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
# ==========================================================
def check_environment() -> Dict[str, Any]:
    """
    LLMç’°å¢ƒã®çŠ¶æ…‹ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹å…±é€šé–¢æ•°
    """
    import os

    env_status = {
        "config_file": YAML_FILE.exists(),
        "env_file": ENV_FILE.exists(),
        "tokens": {
            "gemini": bool(os.getenv("GEMINI_API_KEY")),
            "huggingface": bool(os.getenv("HF_TOKEN") or os.getenv("HUGGINGFACEHUB_API_TOKEN")),
            "ollama": bool(os.getenv("OLLAMA_HOST")) or "localhost_default"
        },
        "config": load_config() if YAML_FILE.exists() else {}
    }

    return env_status


def print_environment_status(debug: bool = False) -> None:
    """
    ç’°å¢ƒçŠ¶æ…‹ã‚’è¡¨ç¤ºã™ã‚‹
    """
    import sys

    status = check_environment()

    print("=== LLM ç’°å¢ƒçŠ¶æ…‹ ===")
    print(f"ğŸ“ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«: {YAML_FILE}")
    print(f"   å­˜åœ¨: {'âœ…' if status['config_file'] else 'âŒ'}")

    print(f"ğŸ“ ç’°å¢ƒãƒ•ã‚¡ã‚¤ãƒ«: {ENV_FILE}")
    print(f"   å­˜åœ¨: {'âœ…' if status['env_file'] else 'âŒ'}")

    print("ğŸ”‘ API ãƒˆãƒ¼ã‚¯ãƒ³:")
    for provider, has_token in status['tokens'].items():
        if has_token == "localhost_default":
            print(f"   {provider}: ğŸ”„ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: localhost)")
        else:
            print(f"   {provider}: {'âœ…' if has_token else 'âŒ'}")

    if debug and status['config']:
        print("\nğŸ“‹ è¨­å®šå†…å®¹:")
        for key, value in status['config'].items():
            print(f"   {key}: {value}")

    print()


# ==========================================================
# å…±é€šLLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼åŸºåº•ã‚¯ãƒ©ã‚¹
# ==========================================================
class LLMProviderConfig:
    """
    LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®å…±é€šåŸºåº•ã‚¯ãƒ©ã‚¹

    æ³¨æ„ï¼šå„ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã¯ç‹¬è‡ªã®è©³ç´°å®Ÿè£…ã‚’ç¶­æŒã§ãã¾ã™
    å˜ä½“ãƒ†ã‚¹ãƒˆã‚„è©³ç´°ãªæƒ…å ±å–å¾—ã®ãŸã‚ã€å…±é€šåŒ–ã—ã™ããªã„ã‚ˆã†ã«ã—ã¾ã™
    """

    def __init__(self, provider_name: str):
        self.provider_name = provider_name
        self.cfg = load_config()

    def get_model_from_config(self, default_model: str) -> str:
        """config.yamlã‹ã‚‰ãƒ¢ãƒ‡ãƒ«åã‚’å–å¾—"""
        return get_llm_model_from_config(self.cfg, self.provider_name, default_model)

    def is_configured(self) -> bool:
        """è¨­å®šãŒæœ‰åŠ¹ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆã‚µãƒ–ã‚¯ãƒ©ã‚¹ã§å®Ÿè£…ï¼‰"""
        raise NotImplementedError

    def get_api_url(self, model: str = None) -> str:
        """API URLã‚’ç”Ÿæˆï¼ˆã‚µãƒ–ã‚¯ãƒ©ã‚¹ã§å®Ÿè£…ï¼‰"""
        raise NotImplementedError

    def build_payload(self, text: str, opts: Dict[str, Any] = None) -> Dict[str, Any]:
        """ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒšã‚¤ãƒ­ãƒ¼ãƒ‰ã‚’æ§‹ç¯‰ï¼ˆã‚µãƒ–ã‚¯ãƒ©ã‚¹ã§å®Ÿè£…ï¼‰"""
        raise NotImplementedError

    def format_error_hint(self, status_code: int, response_text: str, url: str, model: str) -> str:
        """APIã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ’ãƒ³ãƒˆã‚’ç”Ÿæˆ"""
        hints = []

        if status_code == 401:
            hints.append(f"ğŸ”‘ èªè¨¼ã‚¨ãƒ©ãƒ¼: {self.provider_name.upper()}_API_KEY ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
        elif status_code == 403:
            hints.append(f"ğŸš« ã‚¢ã‚¯ã‚»ã‚¹æ‹’å¦: API ã‚­ãƒ¼ã®æ¨©é™ã¾ãŸã¯ãƒ—ãƒ©ãƒ³ã‚’ç¢ºèªã—ã¦ãã ã•ã„")
        elif status_code == 404:
            hints.append(f"ğŸ” ãƒªã‚½ãƒ¼ã‚¹æœªç™ºè¦‹:")
            hints.append(f"   ãƒ¢ãƒ‡ãƒ«: {model}")
            hints.append(f"   URL: {url}")
            hints.append(f"   â†’ ãƒ¢ãƒ‡ãƒ«åãŒæ­£ã—ã„ã‹ç¢ºèªã—ã¦ãã ã•ã„")
        elif status_code == 429:
            hints.append(f"â° ãƒ¬ãƒ¼ãƒˆåˆ¶é™: ã—ã°ã‚‰ãå¾…ã£ã¦ã‹ã‚‰å†è©¦è¡Œã—ã¦ãã ã•ã„")
        elif status_code == 500:
            hints.append(f"ğŸ”§ ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼: {self.provider_name} å´ã®å•é¡Œã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
        elif status_code >= 400:
            hints.append(f"âš ï¸  ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚¨ãƒ©ãƒ¼ (HTTP {status_code})")
            hints.append(f"   ãƒ¢ãƒ‡ãƒ«: {model}")
            hints.append(f"   URL: {url}")

        if "invalid" in response_text.lower() or "not found" in response_text.lower():
            hints.append(f"ğŸ’¡ åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’ç¢ºèªã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™")

        return "\n".join(hints)


def make_api_request(url: str, payload: Dict[str, Any], headers: Dict[str, str],
                    timeout: int, provider_config: LLMProviderConfig,
                    model: str, debug_logger: DebugLogger = None) -> LLMResponse:
    """
    å…±é€šAPI ãƒªã‚¯ã‚¨ã‚¹ãƒˆå‡¦ç†ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒŠãƒ«ï¼‰

    å„ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã¯ç‹¬è‡ªã®è©³ç´°å®Ÿè£…ã‚’ç¶­æŒã§ãã¾ã™ã€‚
    ã“ã®é–¢æ•°ã¯å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã¨ã—ã¦æä¾›ã•ã‚Œã¾ã™ãŒã€
    å˜ä½“ãƒ†ã‚¹ãƒˆã‚„è©³ç´°æƒ…å ±å–å¾—ã®ãŸã‚ã€ä½¿ç”¨ã¯å¿…é ˆã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚

    æˆ»ã‚Šå€¤: LLMResponse ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
    """
    import requests

    start_time = time.time()

    try:
        if debug_logger:
            debug_logger.dbg("POST", url)
            debug_logger.dbg("payload", json.dumps(payload, ensure_ascii=False))

        resp = requests.post(url, json=payload, headers=headers, timeout=timeout)
        response_time = time.time() - start_time

        if resp.status_code != 200:
            error_hint = provider_config.format_error_hint(
                resp.status_code, resp.text, url, model
            )
            error_msg = f"HTTP {resp.status_code}:\n{error_hint}\n\nResponse: {resp.text}"

            return create_llm_response(
                status_code=resp.status_code,
                provider=provider_config.provider_name,
                model=model,
                content="",
                error=error_msg,
                response_time=response_time,
                request_url=url,
                request_payload=payload,
                raw_response={"status_code": resp.status_code, "text": resp.text}
            )

        data = resp.json()

        return create_llm_response(
            status_code=200,
            provider=provider_config.provider_name,
            model=model,
            content="",  # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼å›ºæœ‰ã®å‡¦ç†ã§è¨­å®šã•ã‚Œã‚‹
            response_time=response_time,
            request_url=url,
            request_payload=payload,
            raw_response=data
        )

    except Exception as e:
        response_time = time.time() - start_time
        error_msg = f"Request failed: {e}\nURL: {url}\nModel: {model}"

        return create_llm_response(
            status_code=500,
            provider=provider_config.provider_name,
            model=model,
            content="",
            error=error_msg,
            response_time=response_time,
            request_url=url,
            request_payload=payload
        )
