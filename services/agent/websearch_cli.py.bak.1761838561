#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
services/agent/search_cli.py
CLIベースのメタ検索 & 追跡閲覧ツール
- 検索API: Google CSE / Bing Web Search / SerpAPI / DuckDuckGo(非公式: ライブラリ)
- 一覧表示→番号選択→本文抽出(readability)→再検索(深さ制限)
- robots.txt 準拠、requests_cache によるキャッシュ、JSON/YAML保存

依存:
  pip install requests requests-cache beautifulsoup4 lxml readability-lxml tabulate duckduckgo-search PyYAML
(任意) 対話選択UI:
  pip install InquirerPy

環境変数:
  GOOGLE_API_KEY, GOOGLE_CSE_ID
  BING_API_KEY
  SERPAPI_API_KEY
"""

from __future__ import annotations
import os, sys, time, json, argparse, textwrap, hashlib, re
import urllib.parse as urlparse
import requests
try:
    import requests_cache
    _HAS_CACHE = True
except Exception:
    _HAS_CACHE = False
from bs4 import BeautifulSoup
from tabulate import tabulate
from typing import List, Dict, Any, Optional, Tuple
import yaml

# 任意: duckduckgo-search ライブラリ（無ければ後でフォールバック扱い）
try:
    from duckduckgo_search import DDGS  # type: ignore
    _HAS_DDG = True
except Exception:
    _HAS_DDG = False

# 任意: InquirerPy (あれば選択UI)
try:
    from InquirerPy import inquirer  # type: ignore
    _HAS_INQUIRER = True
except Exception:
    _HAS_INQUIRER = False

# readability
from readability import Document  # type: ignore
from urllib import robotparser

DEFAULT_ENGINES = ["google", "bing", "serpapi", "ddg"]

def _u(s: str) -> str:
    return s.strip() if s else s

def sha1(s: str) -> str:
    return hashlib.sha1(s.encode("utf-8", "ignore")).hexdigest()[:10]

def install_cache(expire_sec: int = 3600, backend: str = "sqlite", cache_name: str = ".search_cache"):
    (_HAS_CACHE and requests_cache.install_cache(cache_name, backend=backend, expire_after=expire_sec)

def robots_allowed(url: str, user_agent: str = "NeuroHubSearchCLI/1.0") -> bool:
    try:
        parsed = urlparse.urlparse(url)
        robots_url = f"{parsed.scheme}://{parsed.netloc}/robots.txt"
        rp = robotparser.RobotFileParser()
        rp.set_url(robots_url)
        rp.read()
        return rp.can_fetch(user_agent, url)
    except Exception:
        # 失敗時は過度にブロックしないが、念のため True とする（保守方針はプロジェクト要件に合わせて）
        return True

# -------- 検索プロバイダ --------

def search_google_cse(query: str, n: int = 10, timeout: int = 15) -> List[Dict[str, str]]:
    api_key = os.getenv("GOOGLE_API_KEY")
    cse_id = os.getenv("GOOGLE_CSE_ID")
    if not api_key or not cse_id:
        raise RuntimeError("GOOGLE_API_KEY / GOOGLE_CSE_ID が未設定")
    url = "https://www.googleapis.com/customsearch/v1"
    params = {"key": api_key, "cx": cse_id, "q": query, "num": min(n, 10)}
    r = requests.get(url, params=params, timeout=timeout)
    r.raise_for_status()
    data = r.json()
    items = []
    for it in data.get("items", []):
        items.append({
            "title": it.get("title") or "",
            "url": it.get("link") or "",
            "snippet": it.get("snippet") or "",
            "source": "google",
        })
    return items

def search_bing(query: str, n: int = 10, timeout: int = 15) -> List[Dict[str, str]]:
    api_key = os.getenv("BING_API_KEY")
    if not api_key:
        raise RuntimeError("BING_API_KEY が未設定")
    url = "https://api.bing.microsoft.com/v7.0/search"
    headers = {"Ocp-Apim-Subscription-Key": api_key}
    params = {"q": query, "count": n, "textDecorations": False, "textFormat": "Raw"}
    r = requests.get(url, headers=headers, params=params, timeout=timeout)
    r.raise_for_status()
    data = r.json()
    items = []
    for it in data.get("webPages", {}).get("value", []):
        items.append({
            "title": it.get("name") or "",
            "url": it.get("url") or "",
            "snippet": it.get("snippet") or "",
            "source": "bing",
        })
    return items

def search_serpapi(query: str, n: int = 10, timeout: int = 15) -> List[Dict[str, str]]:
    api_key = os.getenv("SERPAPI_API_KEY")
    if not api_key:
        raise RuntimeError("SERPAPI_API_KEY が未設定")
    url = "https://serpapi.com/search.json"
    params = {"engine": "google", "q": query, "api_key": api_key, "num": n}
    r = requests.get(url, params=params, timeout=timeout)
    r.raise_for_status()
    data = r.json()
    items = []
    for it in data.get("organic_results", []):
        items.append({
            "title": it.get("title") or "",
            "url": it.get("link") or "",
            "snippet": it.get("snippet") or "",
            "source": "serpapi",
        })
    return items

def search_ddg(query: str, n: int = 10, timeout: int = 15) -> List[Dict[str, str]]:
    # ライブラリがあれば使う（非公式APIを直叩きするより安定）
    if not _HAS_DDG:
        raise RuntimeError("duckduckgo-search が未インストール")
    results = []
    with DDGS(timeout=timeout) as ddgs:
        for r in ddgs.text(query, max_results=n):
            results.append({
                "title": r.get("title") or "",
                "url": r.get("href") or "",
                "snippet": r.get("body") or "",
                "source": "ddg",
            })
    return results

ENGINE_FUNCS = {
    "google": search_google_cse,
    "bing":   search_bing,
    "serpapi": search_serpapi,
    "ddg":    search_ddg,
}

def meta_search(query: str, engines: List[str], max_results: int, timeout: int) -> List[Dict[str, str]]:
    merged: List[Dict[str, str]] = []
    seen = set()
    for eng in engines:
        try:
            func = ENGINE_FUNCS[eng]
        except KeyError:
            continue
        try:
            res = func(query, n=max_results, timeout=timeout)
        except Exception as e:
            # エンジン個別失敗は握りつぶして次へ
            sys.stderr.write(f"[warn] engine '{eng}' failed: {e}\n")
            continue
        for it in res:
            url = it.get("url") or ""
            if not url or url in seen:
                continue
            seen.add(url)
            merged.append(it)
        if len(merged) >= max_results:
            break
    return merged[:max_results]

# -------- 本文抽出 --------

def fetch_html(url: str, timeout: int = 20, ua: str = "Mozilla/5.0 (X11; Linux) NeuroHubSearchCLI/1.0") -> Tuple[str, str]:
    if not robots_allowed(url):
        raise PermissionError(f"robots.txt によりブロック: {url}")
    headers = {"User-Agent": ua, "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"}
    r = requests.get(url, headers=headers, timeout=timeout)
    r.raise_for_status()
    return r.text, r.url  # body, final_url

def extract_readable(html: str, base_url: str) -> Dict[str, str]:
    doc = Document(html)
    title = _u(doc.short_title()) or ""
    content_html = doc.summary(html_partial=True)
    soup = BeautifulSoup(content_html, "lxml")
    # テキスト整形
    text = re.sub(r"\s+\n", "\n", soup.get_text("\n"))
    text = re.sub(r"\n{3,}", "\n\n", text).strip()
    return {"title": title, "text": text, "base_url": base_url}

# -------- 表示/保存 --------

def print_table(results: List[Dict[str, str]]):
    rows = []
    for i, it in enumerate(results, 1):
        rows.append([i, it.get("source", ""), it.get("title", ""), it.get("url", ""), it.get("snippet", "")])
    print(tabulate(rows, headers=["#", "src", "title", "url", "snippet"], tablefmt="github"))

def save_dump(payload: Dict[str, Any], json_path: Optional[str], yaml_path: Optional[str]):
    if json_path:
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(payload, f, ensure_ascii=False, indent=2)
    if yaml_path:
        with open(yaml_path, "w", encoding="utf-8") as f:
            yaml.safe_dump(payload, f, allow_unicode=True, sort_keys=False)

# -------- 対話ループ --------

def interactive_pick(n: int) -> int:
    if _HAS_INQUIRER:
        idx = inquirer.number(
            message="開く番号を入力 (0で終了):",
            min_allowed=0,
            max_allowed=n,
            default=0,
        ).execute()
        return int(idx)
    else:
        while True:
            s = input("開く番号を入力 (0で終了): ").strip()
            if not s.isdigit():
                print("数字を入力してください。")
                continue
            idx = int(s)
            if 0 <= idx <= n:
                return idx

def main():
    ap = argparse.ArgumentParser(
        prog="search_cli",
        formatter_class=argparse.RawTextHelpFormatter,
        description=textwrap.dedent("""
        メタ検索 → 一覧表示 → 選択 → 本文抽出 → 再検索 を行うCLI。
        例:
          python services/agent/search_cli.py "Python web scraping" --pretty
          python services/agent/search_cli.py "VRChat UdonSharp voice" --max-results 15 --engines ddg,bing --depth 3 --json out.json
        """).strip()
    )
    ap.add_argument("query", help="検索ワード（クオート推奨）")
    ap.add_argument("--engines", default=",".join(DEFAULT_ENGINES), help=f"使用エンジン（カンマ区切り）: {','.join(DEFAULT_ENGINES)}")
    ap.add_argument("--max-results", type=int, default=10, help="最大取得件数（マージ後）")
    ap.add_argument("--timeout", type=int, default=15, help="HTTPタイムアウト秒")
    ap.add_argument("--depth", type=int, default=1, help="リンク先本文を辿る最大深さ（0=辿らない）")
    ap.add_argument("--follow-as-query", action="store_true", help="抽出本文の要約(先頭部分)を次クエリとして再検索")
    ap.add_argument("--pretty", action="store_true", help="結果をテーブル表示")
    ap.add_argument("--json", help="結果ダンプ(JSON)")
    ap.add_argument("--yaml", help="結果ダンプ(YAML)")
    ap.add_argument("--no-cache", action="store_true", help="HTTPキャッシュを無効化")
    args = ap.parse_args()

    if not args.no_cache:
        install_cache(expire_sec=3600)

    engines = [e.strip() for e in args.engines.split(",") if e.strip()]

    cur_query = args.query
    all_history = []

    for d in range(args.depth or 1):
        results = meta_search(cur_query, engines, args.max_results, args.timeout)
        if args.pretty:
            print_table(results)
        else:
            print(json.dumps(results, ensure_ascii=False, indent=2))

        payload_block = {"depth": d, "query": cur_query, "results": results}
        all_history.append(payload_block)

        if d == (args.depth - 1):
            break

        if not results:
            print("[info] 結果なし。終了します。")
            break

        idx = interactive_pick(len(results))
        if idx == 0:
            break

        pick = results[idx - 1]
        pick_url = pick.get("url")
        if not pick_url:
            print("[warn] URLが空です。終了します。")
            break

        try:
            html, final_url = fetch_html(pick_url, timeout=args.timeout)
            art = extract_readable(html, final_url)
            print("\n" + "="*80)
            print(f"# {art.get('title','')}\n")
            print(art.get("text","")[:4000])  # 控えめに
            print("\n" + "="*80 + "\n")
        except Exception as e:
            sys.stderr.write(f"[warn] 抽出失敗: {e}\n")
            art = {"title": "", "text": "", "base_url": pick_url}

        payload_block["picked"] = {"url": pick_url, "article": art}

        if args.follow_as_query:
            # 本文の先頭 ~400 文字程度を次クエリ化
            seed = art.get("text", "").strip()
            if not seed:
                print("[info] 本文が空のため再検索をスキップ")
                break
            seed = re.sub(r"\s+", " ", seed)[:400]
            cur_query = seed
            print(f"[info] 次の再検索クエリ（先頭抜粋）: {cur_query[:120]}...")
        else:
            # 通常は次深度も同じクエリで継続
            pass

    # 保存
    if args.json or args.yaml:
        save_dump({"history": all_history, "engines": engines, "ts": int(time.time())}, args.json, args.yaml)

if __name__ == "__main__":
    main()
