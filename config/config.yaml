# AUTO-GENERATED by gen_config_abs.sh (2025-10-28T14:48:47+09:00)
# NOTE: provider_order は availability に基づき Gemini -> Hugging Face -> Ollama の順で整列
llm:
  provider_order:
    - none
  selected_provider: none

  gemini:
    enabled: 0
    api_url: "https://generativelanguage.googleapis.com/v1"
    model: "gemini-2.5-flash"

  huggingface:
    enabled: 0
    api_base: "https://router.huggingface.co"
    chat_completions_url_template: "https://router.huggingface.co/hf-inference/models/{model}/v1/chat/completions"
    inference_api_url_template: "https://api-inference.huggingface.co/models/{model}"
    selected_model: "Qwen/Qwen2.5-0.5B-Instruct"
    models:
      - Qwen/Qwen2.5-0.5B-Instruct
      - google/gemma-2-2b-it
      - HuggingFaceH4/zephyr-7b-beta

  ollama:
    enabled: 0
    host: "http://127.0.0.1:11434"
    selected_model: ""
    models: []
    # 任意のチューニング項目（必要に応じて上書き）
    cpu_model: ""
    memory_mb: 950
    x11_model: "pc105"
